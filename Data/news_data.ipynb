{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import re\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수 세팅\n",
    "corp_file = '/home/yikyungkim/kpmg/fssdata/dart_corpCodes.csv'\n",
    "output_path = '/home/yikyungkim/kpmg/newsdata/news.json'\n",
    "\n",
    "max_page = 100    # 불러올 페이지 수\n",
    "years = 3         # 3년전 기사는 불러오지 않음\n",
    "# start_date = '2020.01.01'\n",
    "# end_date = '2023.02.01'\n",
    "\n",
    "listed = True   # 상장사/비상장사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def news_crawler(query, maxpage):\n",
    "    # 검색어를 바탕으로 크롤링 할 최대 페이지에 해당하는 뉴스 링크를 가져옴\n",
    "        \n",
    "#     start_date_2 = start_date.replace(\".\", \"\")\n",
    "#     end_date_2 = end_date.replace(\".\", \"\")\n",
    "    \n",
    "    start_pg = 1\n",
    "    end_pg = (int(maxpage)-1)*10+1 \n",
    "\n",
    "    naver_urls = []   \n",
    "    \n",
    "    while start_pg < end_pg:\n",
    "        \n",
    "        url = \"https://search.naver.com/search.naver?where=news&sm=tab_pge&query=\" + query + \"&start=\" + str(start_pg)\n",
    "        # url = \"https://search.naver.com/search.naver?where=news&sm=tab_pge&query=\" + query + \"&ds=\" + start_date + \"&de=\" + end_date +  \"&nso=so%3Ar%2Cp%3Afrom\" + start_date_2 + \"to\" + end_date_2 + \"%2Ca%3A&start=\" + str(start_pg)\n",
    "        # ua = UserAgent()\n",
    "        # headers = {'User-Agent' : ua.random}\n",
    "\n",
    "        raw = requests.get(url)\n",
    "        cont = raw.content\n",
    "        html = BeautifulSoup(cont, 'html.parser')\n",
    "        \n",
    "        for urls in html.select(\"a.info\"):\n",
    "            try:\n",
    "                if \"news.naver.com\" in urls['href']:\n",
    "                    naver_urls.append(urls['href'])              \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        start_pg += 10\n",
    "        \n",
    "    return naver_urls \n",
    "\n",
    "\n",
    "def get_company_list(file_path):\n",
    "    #회사 목록 리스트에서 상장사, 비상장사 구분하여 가져옴\n",
    "\n",
    "    with open(file_path,'r', encoding='utf-8') as f:\n",
    "        mycsv=csv.reader(f)\n",
    "        companies=[]\n",
    "        for row in mycsv:\n",
    "            company={}\n",
    "            company['corp_code']=row[0]\n",
    "            company['corp_name']=row[1]\n",
    "            company['stock_code']=row[2]\n",
    "            company['modify_date']=row[3]\n",
    "            companies.append(company)\n",
    "        \n",
    "    cor_listed=[]\n",
    "    cor_not_listed=[]\n",
    "    for company in companies:\n",
    "        if company['stock_code'] == ' ':\n",
    "            cor_not_listed.append(company)\n",
    "        else:\n",
    "            cor_listed.append(company)\n",
    "    \n",
    "    if listed:\n",
    "        return cor_listed\n",
    "    else:\n",
    "        return cor_not_listed        \n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # sample test\n",
    "    # max_page = 10\n",
    "    # start_date = '2022.01.01'\n",
    "    # end_date = '2023.01.01'\n",
    "    # query = '삼성전자'\n",
    "\n",
    "    # 회사 리스트 가져오기\n",
    "    listed = get_company_list(corp_file)\n",
    "    \n",
    "    # 뉴스 가져오기 시작\n",
    "    total_news = []\n",
    "    for i in tqdm(range(1,len(listed)), desc=\"company\"):\n",
    "        \n",
    "        # 회사명 가져오기\n",
    "        query = listed[i]['corp_name']\n",
    "\n",
    "        # url 가져오기\n",
    "        total_urls = news_crawler(query, max_page)\n",
    "        total_urls = list(set(total_urls))  #중복제거\n",
    "\n",
    "        # ConnectionError방지\n",
    "        headers = { \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/98.0.4758.102\" }\n",
    "\n",
    "        # total_news=[]\n",
    "        for url in (total_urls):\n",
    "            raw = requests.get(url,headers=headers)\n",
    "            html = BeautifulSoup(raw.text, \"html.parser\")\n",
    "            news={}\n",
    "            pattern1 = '<[^>]*>'\n",
    "            \n",
    "            ## 날짜\n",
    "            try:\n",
    "                html_date = html.select_one(\"div#ct> div.media_end_head.go_trans > div.media_end_head_info.nv_notrans > div.media_end_head_info_datestamp > div > span\")\n",
    "                news_date = html_date.attrs['data-date-time']\n",
    "            except AttributeError:\n",
    "                news_date = html.select_one(\"#content > div.end_ct > div > div.article_info > span > em\")\n",
    "                news_date = re.sub(pattern=pattern1,repl='',string=str(news_date))\n",
    "            \n",
    "            start_year = datetime.now().year - years\n",
    "            try:\n",
    "                news_year = int(news_date[:4])\n",
    "            except ValueError:\n",
    "                if news_year < start_year:\n",
    "                    break \n",
    "            news['dates']=news_date\n",
    "            \n",
    "            \n",
    "            # url\n",
    "            news['url']=url\n",
    "\n",
    "            # 뉴스 제목\n",
    "            title = html.select(\"div#ct > div.media_end_head.go_trans > div.media_end_head_title > h2\")\n",
    "            title = ''.join(str(title))\n",
    "            \n",
    "            # html태그제거\n",
    "            title = re.sub(pattern=pattern1,repl='',string=title)\n",
    "            news['titles']=title\n",
    "\n",
    "            #뉴스 본문\n",
    "            content = html.select(\"div#dic_area\")\n",
    "            content = ''.join(str(content))\n",
    "            \n",
    "            #html태그제거 및 텍스트 다듬기\n",
    "            content = re.sub(pattern=pattern1,repl='',string=content)\n",
    "            pattern2 = '\\n'\n",
    "            content = re.sub(pattern=pattern2,repl='',string=content)\n",
    "            pattern3 = \"\"\"[\\n\\n\\n\\n\\n// flash 오류를 우회하기 위한 함수 추가\\nfunction _flash_removeCallback() {}\"\"\"\n",
    "            content = content.replace(pattern3,'')\n",
    "            news['content']=content\n",
    "\n",
    "            total_news.append(news)\n",
    "    \n",
    "        \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(total_news, f, ensure_ascii=False, indent='\\t')\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4faaedcce67f6f1d15ed75341b6e480333d5b6f09a31b2124b0674921152c034"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
